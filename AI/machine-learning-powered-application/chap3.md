# Chap3. ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ ë§Œë“¤ê¸°

> ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ì´ˆê¸° í”„ë¡œí† íƒ€ì… ì‹œìŠ¤í…œì„ ë§Œë“œëŠ” ê²ƒì´ ëª¨ë¸ ì„±ëŠ¥ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆëŠ” ìµœì„ ì˜ ë°©ë²•!
>
> â­•**ì‹¤í–‰ê°€ëŠ¥í•œ ì†Œí”„íŠ¸ì›¨ì–´**ë¥¼ **ì ì§„ì **ìœ¼ë¡œ ë°œì „ì‹œí‚¤ìâ­•

<img src="https://blog.crisp.se/wp-content/uploads/2016/01/Making-sense-of-MVP-.jpg" alt="pipe" style="zoom:67%;" />

<br/>

## 3.1 ê°€ì¥ ê°„ë‹¨í•œ í”„ë¡œí† íƒ€ì…

- ***ì¶”ë¡ íŒŒì´í”„ë¼ì¸***ì„ í†µí•´ ì‚¬ìš©ìê°€ ëª¨ë¸ì˜ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€ ì ê²€
- ëª¨ë¸ í›ˆë ¨ ëŒ€ì‹  ê°„ë‹¨í•œ ê·œì¹™/ê²½í—˜ë²•ì¹™ì„ í†µí•´ ì‹œì‘
  - EX. HackerRank ì½”ë“œ ìˆ˜í–‰ ì¸¡ì • ëª¨ë¸
    - ê´„í˜¸ ê°œìˆ˜ í™•ì¸í•˜ê¸°  â†’ Abstract Syntax Tres ë¡œ ë” ë³µì¡í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ëª¨ë¸ë§ì— ì´ˆì ì„ ë§ì¶”ëŠ” ì§ê´€ì„ ì–»ì€ ì²« ë‹¨ê³„
  - EX. ë‚˜ë¬´ ê°œìˆ˜ ì¹´ìš´íŠ¸ ëª¨ë¸ 
    - ë…¹ìƒ‰ í”½ì…€ ë¹„ìœ¨ ê¸°ë°˜ ë‚˜ë¬´ ë°€ë„ ì¸¡ì •  â†’ ìš°ê±°ì§„ ìˆ²ì—ì„œëŠ” ì í•©í•˜ì§€ ì•ŠìŒ  â†’ ë°€ì§‘ë˜ì–´ ìˆëŠ” ë‚˜ë¬´ë¥¼ ì¸ì‹í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ì²« ë‹¨ê³„
- ì „ë¬¸ê°€ ì§€ì‹ & ë°ì´í„° íƒìƒ‰ì„ í†µí•´ ê·œì¹™ ê³ ì•ˆ  â†’ ì´ˆê¸° ê°€ì • ê²€ì¦, ë°˜ë³µ (ë¹ ë¥´ê²Œ)
- ì…ë ¥ ë°ì´í„°  â†’ ì „ì²˜ë¦¬  â†’ ê·œì¹™ ì ìš©  â†’ ê²°ê³¼ ì œê³µ íŒŒì´í”„ë¼ì¸ ìƒì„± (**MVP**)
  - Web App, Terminal (Script), ....

<br/>

## 3.2 ë¨¸ì‹ ëŸ¬ë‹ ì—ë””í„° í”„ë¡œí† íƒ€ì…

> ***ì¼ë°˜ì ì¸ êµì • ê¶Œê³  ì‚¬í•­***ì„ í™œìš© â†’ ì¢‹ì€ ì§ˆë¬¸/ë‚˜ìœ ì§ˆë¬¸ì— ëŒ€í•œ ê·œì¹™ì„ ë§Œë“¤ê³ , ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ì!

```python
# ì‚¬ìš©í•˜ê²Œ ë  í•¨ìˆ˜ë“¤
input_text = parse_arguments()
processed = clean_input(input_text)
tokenized_sentences = preprocess_input(processed)
suggestions = get_suggestions(tokenized_sentences)

# ì„¤ì¹˜í•  ëª¨ë“ˆ
pip install argparse
nltk.download('punkt_tab')
pip install pyphen
```



<br/>

#### 3.2.1  ë°ì´í„° íŒŒì‹±ê³¼ ì •ì œ

```python
# íŒŒì‹±
def parse_arguments():
    """
    ê°„ë‹¨í•œ ëª…ë ¹ì¤„ ë§¤ê°œë³€ìˆ˜ íŒŒì„œ
    :return: ìˆ˜ì •í•  í…ìŠ¤íŠ¸
    """
    
    # ArgumentParser ê°ì²´ ìƒì„± - í”„ë¡œê·¸ë¨ì˜ ì¸ìë¥¼ ì²˜ë¦¬í•˜ëŠ” íŒŒì„œ
    parser = argparse.ArgumentParser(
        description="ìˆ˜ì •í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ í•©ë‹ˆë‹¤"  # í”„ë¡œê·¸ë¨ ì„¤ëª… ì¶”ê°€
    )

    # ìœ„ì¹˜ ì¸ì(positional argument) ì¶”ê°€
    parser.add_argument(
        'text',                # ì¸ì ì´ë¦„
        metavar='input_text',  # ë„ì›€ë§ì— í‘œì‹œë  ì¸ì ì´ë¦„
        type=str              # ì…ë ¥ê°’ì„ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
    )

    # ì…ë ¥ë°›ì€ ì¸ìë“¤ì„ íŒŒì‹±
    args = parser.parse_args()
    
    # íŒŒì‹±ëœ text ì¸ì ê°’ ë°˜í™˜
    return args.text
```



<br/>

```python
# ì •ì œ
def clean_input(text):
    """
    í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜
    :param text: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸
    :return: ASCII ì´ì™¸ì˜ ë¬¸ìë¥¼ ì œê±°í•œ ì •ì œëœ í…ìŠ¤íŠ¸
    """
    # ASCII ë¬¸ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
    # text.encode() : ë¬¸ìì—´ì„ UTF-8 ë°”ì´íŠ¸ë¡œ ì¸ì½”ë”©
    # decode('ascii', errors='ignore') : 
    #     - ASCIIë¡œ ë””ì½”ë”©í•˜ë©´ì„œ ASCIIê°€ ì•„ë‹Œ ë¬¸ìëŠ” ë¬´ì‹œ
    #     - errors='ignore' ì˜µì…˜ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ë¬¸ì ê±´ë„ˆëœ€
    return str(text.encode().decode('ascii', errors='ignore'))
```

```python
text = "Hello ì•ˆë…• World! ğŸŒ"
cleaned = clean_input(text)
print(cleaned)  # ì¶œë ¥: Hello World!
```

<img src="https://github.com/silverpoodle/typora-images/blob/main/pi1.png?raw=true" alt="ã…”ã…‘1" style="zoom:80%;" />

<br/>

#### 3.2.2 í…ìŠ¤íŠ¸ í† í°í™” (Tokenization)

> ë‹¨ì–´ ìˆ˜ì¤€ì˜ í†µê³„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë¥¼ ì‹ë³„
>
> - ê³µë°±/êµ¬ë‘ì  ê¸°ì¤€ìœ¼ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¡œ ë‚˜ëˆ” â†’  ì‹¤ì œë¡œ ì ìš©í•˜ê¸°ëŠ” ì–´ë ¤ì›€
> - ğŸ’¡**NLTK** (ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬) ì‚¬ìš©!

```python
def preprocess_input(text):
   """
   ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤
   :param text: ì •ì œëœ í…ìŠ¤íŠ¸
   :return: ë¬¸ì¥ê³¼ ë‹¨ì–´ë¡œ í† í°í™”í•˜ì—¬ ë¶„ì„ì— íˆ¬ì…í•  ì¤€ë¹„ë¥¼ ë§ˆì¹œ í…ìŠ¤íŠ¸
   """
   # nltk.sent_tokenize(): í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
   # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì„ êµ¬ë¶„
   sentences = nltk.sent_tokenize(text)

   # ê° ë¬¸ì¥ì„ ë‹¤ì‹œ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
   # nltk.word_tokenize(): ê³µë°±, êµ¬ë‘ì  ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ ë¶„ë¦¬
   # ëª¨ë“  ë¬¸ì¥ì— ëŒ€í•´ ë‹¨ì–´ í† í°í™” ìˆ˜í–‰ (sentence in sentences = List Comprehension)
   tokens = [nltk.word_tokenize(sentence) for sentence in sentences]

   return tokens
```

```python
text = "Hello! How are you? I am doing well."
result = preprocess_input(text)
print(cleaned)
```

<img src="https://github.com/silverpoodle/typora-images/blob/main/pi2.png?raw=true" alt="ã…”ã…‘1" style="zoom:80%;" />

<br/>

#### 3.2.3 íŠ¹ì„± ìƒì„±í•˜ê¸°

- ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•œ ì¡°ì–¸ì„ ìœ„í•œ ê·œì¹™ ìƒì„±
  - ìì£¼ ì‚¬ìš©ë˜ëŠ” ë™ì‚¬, ì—°ê²°ì–´ì˜ ë¹ˆë„ count
  - wh- ì ‘ì†ì‚¬ë¥¼ ì¹´ìš´íŠ¸ (why what where who..)
  - Flesh ê°€ë…ì„± ì ìˆ˜ ê³„ì‚°

- ìœ„ í†µê³„ê°’ì„ ëª¨ì•„ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬!!

```python
import pyphen

def compute_flesch_reading_ease(total_syllables, total_words, total_sentences):
    """
    ìš”ì•½ í†µê³„ë¡œë¶€í„° ê°€ë…ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    :param total_syllables: ì…ë ¥ í…ìŠ¤íŠ¸ì— ìˆëŠ” ìŒì ˆ ê°œìˆ˜
    :param total_words: ì…ë ¥ í…ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ ê°œìˆ˜
    :param total_sentences: ì…ë ¥ í…ìŠ¤íŠ¸ì— ìˆëŠ” ë¬¸ì¥ ê°œìˆ˜
    :return: A readability score: ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ë” ë³µì¡í•œ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    """
    return (
        206.85
        - 1.015 * (total_words / total_sentences)
        - 84.6 * (total_syllables / total_words)
    )


def get_reading_level_from_flesch(flesch_score):
    """
    https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests ì—ì„œ ê°€ì ¸ì˜¨ ì„ê³—ê°’
    :param flesch_score:
    :return: í”Œë ˆì‹œ ì ìˆ˜ì— ëŒ€í•œ ê°€ë…ì„± ìˆ˜ì¤€
    """
    if flesch_score < 30:
        return "ë§¤ìš° ì½ê¸° ì–´ë ¤ì›€"
    elif flesch_score < 50:
        return "ì½ê¸° ì–´ë ¤ì›€"
    elif flesch_score < 60:
        return "ì•½ê°„ ì½ê¸° ì–´ë ¤ì›€"
    elif flesch_score < 70:
        return "ë³´í†µ"
    elif flesch_score < 80:
        return "ì•½ê°„ ì½ê¸° ì‰¬ì›€"
    elif flesch_score < 90:
        return "ì½ê¸° ì‰¬ì›€"
    else:
        return "ë§¤ìš° ì½ê¸° ì‰¬ì›€"


def compute_average_word_length(tokens):
    """
    í•œ ë¬¸ì¥ì— ìˆëŠ” ë‹¨ì–´ì˜ ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    :param tokens: ë‹¨ì–¼ ë¦¬ìŠ¤íŠ¸
    :return: ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ì˜ í‰ê·  ê¸¸ì´
    """
    word_lengths = [len(word) for word in tokens]
    return sum(word_lengths) / len(word_lengths)


def compute_total_average_word_length(sentence_list):
    """
    ì—¬ëŸ¬ ë¬¸ì¥ì— ëŒ€í•œ ë‹¨ì–´ì˜ í‰ê·  ê¸¸ì´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    :param sentence_list: ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    :return: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ì˜ í‰ê·  ê¸¸ì´
    """
    lengths = [compute_average_word_length(tokens) for tokens in sentence_list]
    return sum(lengths) / len(lengths)


def compute_total_unique_words_fraction(sentence_list):
    """
    ê³ ìœ í•œ ë‹¨ì–´ì˜ ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    :param sentence_list: ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    :return: ë¬¸ì¥ì— ìˆëŠ” ê³ ìœ í•œ ë‹¨ì–´ì˜ ë¹„ìœ¨
    """
    all_words = [word for word_list in sentence_list for word in word_list]
    unique_words = set(all_words)
    return len(unique_words) / len(all_words)


def count_word_usage(tokens, word_list):
    """
    ì£¼ì–´ì§„ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì˜ ë“±ì¥ íšŸìˆ˜
    :param tokens: í•œ ë¬¸ì¥ì˜ í† í° ë¦¬ìŠ¤íŠ¸
    :param word_list: íƒìƒ‰í•˜ë ¤ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    :return: ë¦¬ìŠ¤íŠ¸ì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ íšŸìˆ˜
    """
    return len([word for word in tokens if word.lower() in word_list])


def count_word_syllables(word):
    """
    ë‹¨ì–´ì— ìˆëŠ” ìŒì ˆ íšŸìˆ˜
    :param word: í•˜ë‚˜ì˜ ë‹¨ì–´ ë¬¸ìì—´
    :return: pyphenìœ¼ë¡œ êµ¬í•œ ìŒì ˆ ê°œìˆ˜
    """
    dic = pyphen.Pyphen(lang="en_US")
    # ìŒì ˆ ì‚¬ì´ì— í•˜ì´í”ˆ("-")ì„ ì¶”ê°€í•œ ë‹¨ì–´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    hyphenated = dic.inserted(word)
    return len(hyphenated.split("-"))


def count_sentence_syllables(tokens):
    """
    ë¬¸ì¥ì— ìˆëŠ” ìŒì ˆ ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤.
    :param tokens: ë‹¨ì–´ì™€ êµ¬ë‘£ì ì˜ ë¦¬ìŠ¤íŠ¸
    :return: ë¬¸ì¥ì— ìˆëŠ” ìŒì ˆ ê°œìˆ˜
    """
    # í† í°í™” ê°ì²´ëŠ” êµ¬ë‘£ì ì„ ë³„ë„ì˜ ë‹¨ì–´ë¡œ ì¸ì‹í•˜ê¸° ë•Œë¬¸ì— ì—¬ê¸°ì„œëŠ” ì´ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    punctuation = ".,!?/"
    return sum(
        [
            count_word_syllables(word)
            for word in tokens
            if word not in punctuation
        ]
    )


def count_total_syllables(sentence_list):
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ìŒì ˆì„ ì…‰ë‹ˆë‹¤.
    :param sentence_list: ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    :return: ë¬¸ì¥ì— ìˆëŠ” ìŒì ˆì˜ ê°œìˆ˜
    """
    return sum(
        [count_sentence_syllables(sentence) for sentence in sentence_list]
    )


def count_words_per_sentence(sentence_tokens):
    """
    ë¬¸ì¥ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì…‰ë‹ˆë‹¤.
    :param sentence_tokens: ë‹¨ì–´ì™€ êµ¬ë‘£ì ì˜ ë¦¬ìŠ¤íŠ¸
    :return: ë¬¸ì¥ì— ìˆëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜
    """
    punctuation = ".,!?/"
    return len([word for word in sentence_tokens if word not in punctuation])


def count_total_words(sentence_list):
    """
    ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì…‰ë‹ˆë‹¤.
    :param sentence_list: ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    :return: ë¬¸ì¥ì— ìˆëŠ” ë‹¨ì–´ì˜ ê°œìˆ˜
    """
    return sum(
        [count_words_per_sentence(sentence) for sentence in sentence_list]
    )

def get_suggestions(sentence_list):
   """
   ì¶”ì²œì„ í¬í•¨í•œ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
   :param sentence_list: ë¬¸ì¥ì˜ ë¦¬ìŠ¤íŠ¸. ê° ë¬¸ì¥ì€ ë‹¨ì–´ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
   :return: ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ì¶”ì²œì‚¬í•­ë“¤ì„ í¬í•¨í•œ ë¬¸ìì—´
   """
   # íŠ¹ì • ë‹¨ì–´ë“¤ì˜ ì‚¬ìš© ë¹ˆë„ ê³„ì‚°
   told_said_usage = sum(
       count_word_usage(tokens, ["told", "said"]) 
       for tokens in sentence_list
   )
   
   but_and_usage = sum(
       count_word_usage(tokens, ["but", "and"]) 
       for tokens in sentence_list
   )
   
   # whë¡œ ì‹œì‘í•˜ëŠ” ë¶€ì‚¬ë“¤ì˜ ì‚¬ìš© ë¹ˆë„ ê³„ì‚°
   wh_adverbs_usage = sum(
       count_word_usage(
           tokens,
           ["when", "where", "why", "whence", "whereby", "wherein", "whereupon"]
       ) for tokens in sentence_list
   )
   
   # ê²°ê³¼ ë¬¸ìì—´ ìƒì„± ì‹œì‘
   result_str = ""
   
   # ë¶€ì‚¬ ì‚¬ìš©ì— ëŒ€í•œ í†µê³„ ì¶”ê°€
   adverb_usage = "told/said ì‚¬ìš©: %d, but/and ì‚¬ìš©: %d, wh-ë¶€ì‚¬ ì‚¬ìš©: %d" % (
       told_said_usage,
       but_and_usage,
       wh_adverbs_usage
   )
   result_str += adverb_usage

   # ë‹¨ì–´ ê¸¸ì´ì™€ ë‹¤ì–‘ì„±ì— ëŒ€í•œ í†µê³„ ê³„ì‚°
   average_word_length = compute_total_average_word_length(sentence_list)
   unique_words_fraction = compute_total_unique_words_fraction(sentence_list)
   
   # ë‹¨ì–´ í†µê³„ ë¬¸ìì—´ ìƒì„±
   word_stats = "ë‹¨ì–´ì˜ í‰ê·  ê¸¸ì´: %.2f, ê³ ìœ í•œ ë‹¨ì–´ì˜ ë¹„ìœ¨: %.2f" % (
       average_word_length,
       unique_words_fraction
   )
   result_str += "<br/>"  # HTML ì¤„ë°”ê¿ˆ íƒœê·¸
   result_str += "\n"
   result_str += word_stats

   # ìŒì ˆ, ë‹¨ì–´, ë¬¸ì¥ ìˆ˜ ê³„ì‚°
   number_of_syllables = count_total_syllables(sentence_list)
   number_of_words = count_total_words(sentence_list)
   number_of_sentences = len(sentence_list)
   
   # ê¸°ë³¸ í†µê³„ ë¬¸ìì—´ ìƒì„±
   syllable_counts = "%dê°œ ìŒì ˆ, %dê°œ ë‹¨ì–´, %dê°œ ë¬¸ì¥" % (
       number_of_syllables,
       number_of_words,
       number_of_sentences
   )
   result_str += "<br/>"
   result_str += "\n"
   result_str += syllable_counts

   # Flesch Reading Ease ì ìˆ˜ ê³„ì‚° ë° í•´ì„
   flesch_score = compute_flesch_reading_ease(
       number_of_syllables, 
       number_of_words, 
       number_of_sentences
   )
   
   flesch = "%.2f: %s" % (
       flesch_score,
       get_reading_level_from_flesch(flesch_score)
   )
   result_str += "<br/>"
   result_str += "\n"
   result_str += flesch

   return result_str
```

```python
# ì˜ˆì‹œ í…ìŠ¤íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
text = [
    ["I", "told", "him", "about", "the", "book"],
    ["He", "said", "it", "was", "interesting"],
    ["When", "and", "where", "did", "you", "read", "it"]
]

suggestions = get_suggestions(text)
print(suggestions)
```

<img src="https://github.com/silverpoodle/typora-images/blob/main/image-20241228114008989.png?raw=true" alt="image-20241228114008989" style="zoom:80%;" />

<br/>

## 3.3 Workflow í…ŒìŠ¤íŠ¸í•˜ê¸°

#### 3.3.1 ì‚¬ìš©ì ê²½í—˜

- ëª¨ë¸ì˜ í’ˆì§ˆê³¼ëŠ” ë³„ê°œë¡œ *ì‚¬ìš©ì„±ì˜ ë§Œì¡±ë„* í™•ì¸ = *ê°€ì¥ ë°”ëŒì§í•œ í˜•íƒœ*ë¡œ ê²°ê³¼ë¥¼ ì œê³µí•˜ê³  ìˆëŠ”ê°€

  - ì œì‹œí•œ ê²°ê³¼ê°€ ìœ ìš”í•œê°€ or ëª¨ë¸ì„ ê°œì„ í•´ì•¼ í•˜ëŠ”ê°€

  <br/>

#### 3.3.2 ëª¨ë¸ë§ ê²°ê³¼

- **ì„ íƒí•œ ì¸¡ì •ì§€í‘œ**ë¡œ í‰ê°€ ë°˜ë³µ
  - EX. ë Œí„°ì¹´ ê²€ìƒ‰ ì‹œìŠ¤í…œ â†’ [í• ì¸ ëˆ„ì  ì´ë“] ì§€í‘œ ì‚¬ìš© (ê°€ì¥ ê´€ë ¨ ìˆëŠ” í•­ëª©ì´ ë‹¤ë¥¸ ê²ƒë³´ë‹¤ ë¨¼ì € ë°˜í™˜ë  ì‹œ ë†’ì€ ì ìˆ˜ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆœìœ„ í’ˆì§ˆ ì¸¡ì •) 
    - DCG5(5ê°œ ê²°ê³¼ ì¤‘ ìœ ìš©í•œ ì¶”ì²œì´ ì ì–´ë„ í•˜ë‚˜ëŠ” ìˆì–´ì•¼ í•œë‹¤) â†’ ì‚¬ìš©ìê°€ ì¶œë ¥ 3ê°œë§Œ ê³ ë ¤í•œë‹¤ë©´â“ â†’ DCG3
- **ì„±ëŠ¥ ë³‘ëª©** ì°¾ê¸°
  - ì œí’ˆ ë¶€ë¶„: EX. ì—°êµ¬ ë…¼ë¬¸ ì´ë¯¸ì§€ë¡œ ì½˜í¼ëŸ°ìŠ¤ í†µê³¼ ì˜ˆì¸¡ ëª¨ë¸ â†’ ê±°ì ˆë  í™•ë¥  **+** ë…¼ë¬¸ ê°œì„  ì¡°ì–¸
  - ëª¨ë¸ ë¶€ë¶„: EX. ì‹ ìš© ì ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸  â†’ íŠ¹ì • ì¸ì¢…ì˜ ì²´ë‚© ê°€ëŠ¥ì„± ë†’ê²Œ ì¶œë ¥
    - ì‚¬ìš©í•˜ëŠ” í›ˆë ¨ ë°ì´í„°ê°€ ***í¸í–¥***  â†’ ë°ì´í„° ì •ì œ/ì¦ì‹ íŒŒì´í”„ë¼ì¸ ìƒì„± í•´ì•¼í•¨  â†’ *ëª¨ë¸ ìˆ˜ì •* í•„ìš”

<br/>



## 3.4 ë¨¸ì‹ ëŸ¬ë‹ ì—ë””í„° í”„ë¡œí† íƒ€ì… í‰ê°€

<img src="https://github.com/silverpoodle/typora-images/blob/main/image-20241229152957074.png?raw=true" alt="image-20241229152957074" style="zoom:67%;" />

<br/>

#### 3.4.1 ëª¨ë¸

- ê²°ê³¼ê°€ ì¢‹ì€ í’ˆì§ˆì˜ ê¸€ì“°ê¸°ì™€ ê´€ë ¨ ìˆëŠ”ê°€?  â†’ âŒ
  - ë³µì¡í•œ ë¬¸ì¥ì˜ ê°€ë…ì„± ì ìˆ˜ = ì „ì²´ ë¬¸ë‹¨ì˜ ê°€ë…ì„± ì ìˆ˜  â†’ ì¶”ì¶œëœ ì†ì„±ê°’ì´ 'ì¢‹ì€ ê¸€ì“°ê¸°' ì™€ëŠ” ê´€ë ¨ ì—†ìŒ

#### 3.4.2 ì‚¬ìš©ì ê²½í—˜

- ë°˜í™˜ëœ ì •ë³´ê°€ ë„ˆë¬´ ì¥í™©í•˜ê³  ê´€ë ¨ì„±ì´ ì—†ìŒ
  - ê°€ë…ì„± ì ìˆ˜ëŠ” í’ˆì§ˆ ì§€í‘œì´ì§€ë§Œ, *ì§ˆë¬¸ì„ ê°œì„ í•˜ëŠ”ë°ëŠ” ë„ì›€ì´ ë˜ì§€ ëª»í•¨*
- wh- ì ‘ì†ì‚¬ë¥¼ ì ê²Œ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •ì„ ì œì•ˆí•œë‹¤ë©´â“ì¡°ê¸ˆ ë” ì„¸ë¶€ì ìœ¼ë¡œ ë‹¨ì–´/ë¬¸ì¥ ìˆ˜ì¤€ì˜ ë³€í™”ë¥¼ ì œì•ˆí•œë‹¤ë©´â“
  - EX. I lost Password. What do?
    - ê°€ë…ì„± ë‚˜ì¨ â†’ ì–´ë–¤ ì œí’ˆì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”. ê°€ë…ì„±ì´ ë‚˜ì©ë‹ˆë‹¤.









ì°¸ê³ 

https://blog.crisp.se/2016/01/25/henrikkniberg/making-sense-of-mvp